{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Transfer Learning\n",
    " \n",
    "**Authors:**\n",
    "\n",
    "1.   Liav Bachar 205888472\n",
    "2.   Naor Kolet 205533060\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2z5iHmg0oke"
   },
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pT--0V_r0Q4B"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input as preprocess_input_res\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as preprocess_input_vgg\n",
    "from tensorflow.keras.applications import VGG16, ResNet50V2, EfficientNetB4,  MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import log_loss\n",
    " \n",
    "# Plots\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Misc.\n",
    "from scipy.io import loadmat\n",
    "import os\n",
    "import random\n",
    "import joblib\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "def set_seed():    \n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another dataset? https://www.tensorflow.org/tutorials/load_data/images\n",
    "if not os.path.exists(r'./datasets/'):\n",
    "    !mkdir ./datasets\n",
    "    !wget 'https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz' -P './datasets/'\n",
    "    !tar -xf ./datasets/102flowers.tgz -C ./datasets/\n",
    "    \n",
    "#     !wget 'https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102segmentations.tgz' -P './datasets/'\n",
    "    \n",
    "    !wget 'https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat' -P './datasets/'\n",
    "    \n",
    "labels = loadmat('./datasets/imagelabels.mat')['labels'].reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = glob('./datasets/jpg/*')\n",
    "images_path.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val_test(split_seed):\n",
    "    # train 50% validation 25% test 25%\n",
    "    train_paths, val_tst_paths, train_labels, val_tst_labels = train_test_split(images_path, labels, train_size=0.5, random_state=split_seed, shuffle=True, stratify=labels)\n",
    "    val_paths, tst_paths, val_labels, tst_labels = train_test_split(val_tst_paths, val_tst_labels, train_size=0.5, random_state=split_seed, shuffle=True, stratify=val_tst_labels)\n",
    "    \n",
    "    def convert2df(paths, labels): return pd.DataFrame({'filename': paths, 'class':labels}).astype(str)\n",
    "    train_df = convert2df(train_paths, train_labels)\n",
    "    val_df = convert2df(val_paths, val_labels)\n",
    "    test_df = convert2df(tst_paths, tst_labels)\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data_by_resample(df, per_class=128):\n",
    "\n",
    "    dfs = [df[df['class'] == i] for i in df['class'].unique()]\n",
    "#     per_class = max(per_class, max([df.shape[0] for df in dfs]))\n",
    "\n",
    "    def resample(df):\n",
    "        sample_amt = per_class - df.shape[0]\n",
    "        if sample_amt > 0:\n",
    "            sample_dfs = [df]\n",
    "            while sample_amt:\n",
    "                curr_amt = min(sample_amt, df.shape[0])\n",
    "                sample_df = df.sample(curr_amt)\n",
    "                sample_dfs.append(sample_df)\n",
    "                sample_amt -= curr_amt\n",
    "\n",
    "            return pd.concat(sample_dfs)\n",
    "        \n",
    "        if sample_amt < 0:\n",
    "            return df.sample(per_class)\n",
    "\n",
    "        return df\n",
    "\n",
    "    dfs_resample = [resample(df) for df in dfs]\n",
    "    \n",
    "    return pd.concat(dfs_resample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagen_flow(df, batch_size, fe_type, subset):\n",
    "    classes = list(np.unique(labels).astype(str))\n",
    "    if fe_type == 'resnet':\n",
    "        preprocess_func = preprocess_input_res\n",
    "        resize_shape = (224, 224)\n",
    "    if fe_type == 'vgg':\n",
    "        preprocess_func = preprocess_input_vgg\n",
    "        resize_shape = (224, 224)\n",
    "    \n",
    "    \n",
    "    if subset == 'train':\n",
    "        data_gen = ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "#              rescale=1./255\n",
    "            preprocessing_function=preprocess_func\n",
    "           )\n",
    "    else:\n",
    "        data_gen = ImageDataGenerator(preprocessing_function=preprocess_func)\n",
    "        \n",
    "    return data_gen.flow_from_dataframe(df, batch_size=batch_size, target_size=resize_shape, seed=SEED, validate_filenames=False, classes=classes, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_extractor(base_model, width=224, height=224, channel=3):\n",
    "    model = base_model(input_shape=(width, height, channel), include_top=False)\n",
    "    \n",
    "    trainable = False\n",
    "    for layer in model.layers:\n",
    "        if base_model.__name__ == 'ResNet50V2' and 'conv5' in layer.name:\n",
    "            trainable = True\n",
    "            \n",
    "        layer.trainable = trainable\n",
    "        \n",
    "            \n",
    "    return Model(model.input, model.output, name=base_model.__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "output_shape = 102"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Here we build our model architecture. It contains 4 main parts.\n",
    "\n",
    "1. **Embedding Layer** - Here the words of each song will be converted to their embeddings representations. We made this layer trainable, let the model learn the embeddings for the missing words.\n",
    "2. **Concatenate Layer** - Here we attach for each word entry the related melody record\n",
    "3. **LSTM Layers** - Here we created two layers of lstm, the first one learn features across the windows and outputs all the hidden states, the next one gets those hidden states and output a single feature that was learnt from them.\n",
    "4. **Softmax Layer** - Give each word in our dictionary the probabilty it will be the next word after the window.\n",
    "\n",
    "Note: We are using Dropout layers in order to reduce the overfitting -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(model_name):\n",
    "    acc = 'val_acc'\n",
    "    acc_mode = 'max'\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "                              fr'./models/{model_name}.h5', \n",
    "                              monitor=acc, \n",
    "#                               verbose=1, \n",
    "                              save_best_only=True, \n",
    "                              mode=acc_mode)\n",
    "    earlystop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "    reduceLR = ReduceLROnPlateau(monitor = 'val_loss', mode = 'min', patience = 3,\n",
    "                            factor = 0.5, min_lr = 1e-6, verbose = 1)\n",
    "\n",
    "    return [checkpoint, reduceLR]  #earlystop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model_gen, fe_type, train_df, val_df, use_saved=False, params_dict=None):\n",
    "    os.makedirs('./models', exist_ok=True)\n",
    "    params = ''\n",
    "    if params_dict is not None:\n",
    "        params = '_'.join(f'{key}_{val}' for key,val in params_dict.items())\n",
    "    model_name = f'{fe_type}' + f'_{params}'\n",
    "        \n",
    "    if use_saved:\n",
    "        history = joblib.load(fr'./models/{model_name}_history.sav')\n",
    "    else:\n",
    "        callbacks = get_callbacks(model_name)\n",
    "        \n",
    "        train_flow = datagen_flow(train_df, params_dict['batch_size'], fe_type, 'train')\n",
    "        val_flow = datagen_flow(val_df, params_dict['batch_size'], fe_type, 'val')\n",
    "        \n",
    "        model = model_gen(image_shape, output_shape, fe_type)\n",
    "#         model = load_model(fr'./models/{model_name}.h5')\n",
    "        history = model.fit(\n",
    "                            x=train_flow,\n",
    "                            y=None,\n",
    "                            batch_size=None,\n",
    "                            epochs=params_dict['epochs'],\n",
    "                            validation_data=val_flow,\n",
    "                            callbacks=callbacks,\n",
    "                            steps_per_epoch = params_dict['steps'],\n",
    "                            validation_steps = params_dict['validation_steps'],\n",
    "                            workers=5\n",
    "                            )\n",
    "        \n",
    "        history = history.history\n",
    "        joblib.dump(history, fr'./models/{model_name}_history.sav')\n",
    "    \n",
    "    model = load_model(fr'./models/{model_name}.h5')\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_perf(history):\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(5*2,5))\n",
    "    fig.suptitle(f'Model performance over epochs')\n",
    "    \n",
    "    for k in ['loss', 'val_loss']:\n",
    "        ax[0].plot(history[k])\n",
    "        \n",
    "    ax[0].legend(['train_loss', 'val_loss'])\n",
    "    ax[0].margins(0.01)\n",
    "    ax[0].set_title('Crossentropy')\n",
    "    \n",
    "    for k in ['acc', 'val_acc']:\n",
    "        ax[1].plot(history[k])\n",
    "        \n",
    "    ax[1].legend(['train_accuracy', 'val_accuracy'])\n",
    "    ax[1].margins(0.01)\n",
    "    ax[1].set_title('Accuracy')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def init_model(image_shape, output_shape, fe_type='vgg'):\n",
    "    inp = Input(shape=image_shape, name='image')\n",
    "    \n",
    "    if fe_type == 'vgg':\n",
    "        X = get_feature_extractor(VGG16, width=224, heigh=224)(inp)\n",
    "    elif fe_type == 'resnet':\n",
    "        X = get_feature_extractor(ResNet50V2, width=224, height=224)(inp)\n",
    "    elif fe_type == 'mobile_net':\n",
    "        X = get_feature_extractor(MobileNetV2, width=160, height=160)(inp)\n",
    "    else:\n",
    "        X = inp\n",
    "    \n",
    "    \n",
    "    X = MaxPool2D(pool_size=(3, 3))(X)\n",
    "    X = Flatten()(X)\n",
    "#     X = Dropout(0.5)(X)\n",
    "    X = Dense(512, activation=\"relu\",  kernel_regularizer=tf.keras.regularizers.L2(0.01))(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(256, activation=\"relu\",  kernel_regularizer=tf.keras.regularizers.L2(0.01))(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    out = Dense(output_shape, activation=\"softmax\", name = 'out')(X)\n",
    "\n",
    "    model = Model(inp, out)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=1e-3), metrics=['acc'])\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1\n",
      "Found 4094 non-validated image filenames belonging to 102 classes.\n",
      "Found 2047 non-validated image filenames belonging to 102 classes.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4a8e9b50291a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mimage_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mresnet_histories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'resnet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0mvisualize_perf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet_histories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-4a8e9b50291a>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model_gen, fe_type, k, params_dict)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mparams_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mparams_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mparams_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation_steps'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mparams_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mcurr_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfe_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-142bc40c0ed0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_gen, fe_type, train_df, val_df, use_saved, params_dict)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mval_flow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatagen_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfe_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m#         model = load_model(fr'./models/{model_name}.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         history = model.fit(\n",
      "\u001b[0;32m<ipython-input-13-f95b8d3c428d>\u001b[0m in \u001b[0;36minit_model\u001b[0;34m(image_shape, output_shape, fe_type)\u001b[0m\n\u001b[1;32m      5\u001b[0m        \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_feature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVGG16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m    \u001b[0;32melif\u001b[0m \u001b[0mfe_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'resnet'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m        \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_feature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResNet50V2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m    \u001b[0;32melif\u001b[0m \u001b[0mfe_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mobile_net'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m        \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_feature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMobileNetV2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-03ff8a24919b>\u001b[0m in \u001b[0;36mget_feature_extractor\u001b[0;34m(base_model, width, height, channel)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_feature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/liav_env/lib/python3.8/site-packages/tensorflow/python/keras/applications/resnet_v2.py\u001b[0m in \u001b[0;36mResNet50V2\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   return resnet.ResNet(\n\u001b[0m\u001b[1;32m     49\u001b[0m       \u001b[0mstack_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/liav_env/lib/python3.8/site-packages/tensorflow/python/keras/applications/resnet.py\u001b[0m in \u001b[0;36mResNet\u001b[0;34m(stack_fn, preact, use_bias, model_name, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mcache_subdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         file_hash=file_hash)\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/liav_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2232\u001b[0m             f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[1;32m   2233\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2234\u001b[0;31m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2236\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/liav_env/lib/python3.8/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    677\u001b[0m   \u001b[0mfiltered_layer_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m     \u001b[0mweight_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_attributes_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight_names'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/liav_env/lib/python3.8/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid HDF5 object reference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0moid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0motype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5i.pyx\u001b[0m in \u001b[0;36mh5py.h5i.wrap_identifier\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/liav_env/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate_model(model_gen, fe_type, k=5, params_dict=None):\n",
    "    histories = []\n",
    "    losses = []\n",
    "    accs = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        print(f'Training model {i+1}')\n",
    "        \n",
    "        set_seed()\n",
    "        \n",
    "        train_df, val_df, test_df = split_train_val_test(SEED+i)\n",
    "        params_dict['steps'] = train_df.shape[0] // params_dict['batch_size']\n",
    "        params_dict['validation_steps'] = val_df.shape[0] // params_dict['batch_size']\n",
    "        curr_model, curr_history = train_model(model_gen, fe_type, train_df, val_df, params_dict=params_dict)\n",
    "        \n",
    "        \n",
    "        test_flow = datagen_flow(test_df, params_dict['batch_size'], fe_type, 'test')\n",
    "        curr_loss, curr_acc = curr_model.evaluate(test_flow, steps=params_dict['validation_steps'])\n",
    "        \n",
    "        histories.append(curr_history)\n",
    "        losses.append(curr_loss)\n",
    "        accs.append(curr_acc)\n",
    "        \n",
    "        del curr_model\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "    print(f'Test CrossEntropy: {np.mean(losses):.4f} +-{np.std(losses):.4f}')\n",
    "    print(f'Test Accuracy: {np.mean(accs)*100:.2f}% +-{np.std(accs)*100:.2f}%')\n",
    "    \n",
    "    return histories\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visualize_perf(history_per_fold):\n",
    "    k = len(history_per_fold)\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=k, figsize=(k*5,10))\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    fig.suptitle(f'Model performance over epochs')\n",
    "    \n",
    "    for fold_no, h in enumerate(history_per_fold):\n",
    "\n",
    "        for k in ['loss', 'val_loss']:\n",
    "            ax[0,fold_no].plot(h[k])\n",
    "            ax[0,fold_no].legend(['train_loss', 'val_loss'])\n",
    "            ax[0,fold_no].margins(0.01)\n",
    "            ax[0,fold_no].set_title(f'Fold #{fold_no+1} - loss')\n",
    "\n",
    "        for k in ['acc', 'val_acc']:\n",
    "            ax[1,fold_no].plot(h[k])\n",
    "            ax[1,fold_no].legend(['train_acc', 'val_acc'])\n",
    "            ax[1,fold_no].margins(0.01)\n",
    "            ax[1,fold_no].set_title(f'Fold #{fold_no+1} - accuracy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "params_dict = {\n",
    "    'epochs': 20,\n",
    "    'batch_size': 32,\n",
    "    'steps': 127,\n",
    "    'validation_steps':63\n",
    "}\n",
    "\n",
    "image_shape = (224, 224, 3)\n",
    "resnet_histories = evaluate_model(init_model, 'resnet', k=2, params_dict=params_dict)\n",
    "visualize_perf(resnet_histories)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1\n",
      "Found 13056 non-validated image filenames belonging to 102 classes.\n",
      "Found 2047 non-validated image filenames belonging to 102 classes.\n",
      "Epoch 1/20\n",
      "408/408 [==============================] - 81s 192ms/step - loss: 12.2300 - acc: 0.0668 - val_loss: 3.9816 - val_acc: 0.4737\n",
      "Epoch 2/20\n",
      "408/408 [==============================] - 76s 185ms/step - loss: 3.6634 - acc: 0.4915 - val_loss: 2.4868 - val_acc: 0.7316\n",
      "Epoch 3/20\n",
      "408/408 [==============================] - 76s 184ms/step - loss: 2.2412 - acc: 0.7554 - val_loss: 1.8931 - val_acc: 0.8294\n",
      "Epoch 4/20\n",
      "408/408 [==============================] - 75s 182ms/step - loss: 1.6536 - acc: 0.8549 - val_loss: 1.4840 - val_acc: 0.8978\n",
      "Epoch 5/20\n",
      "408/408 [==============================] - 75s 182ms/step - loss: 1.3561 - acc: 0.9045 - val_loss: 1.9640 - val_acc: 0.8636\n",
      "Epoch 6/20\n",
      "408/408 [==============================] - 81s 196ms/step - loss: 1.2729 - acc: 0.9236 - val_loss: 1.4353 - val_acc: 0.9062\n",
      "Epoch 7/20\n",
      "408/408 [==============================] - 75s 183ms/step - loss: 1.1834 - acc: 0.9340 - val_loss: 1.6202 - val_acc: 0.8869\n",
      "Epoch 8/20\n",
      "408/408 [==============================] - 80s 195ms/step - loss: 1.2246 - acc: 0.9336 - val_loss: 1.4804 - val_acc: 0.9043\n",
      "Epoch 9/20\n",
      "408/408 [==============================] - 80s 194ms/step - loss: 1.1785 - acc: 0.9387 - val_loss: 1.3479 - val_acc: 0.9142\n",
      "Epoch 10/20\n",
      "408/408 [==============================] - 76s 185ms/step - loss: 1.0997 - acc: 0.9499 - val_loss: 1.5425 - val_acc: 0.9177\n",
      "Epoch 11/20\n",
      "408/408 [==============================] - 75s 182ms/step - loss: 1.1675 - acc: 0.9452 - val_loss: 1.3282 - val_acc: 0.9256\n",
      "Epoch 12/20\n",
      "232/408 [================>.............] - ETA: 28s - loss: 0.9905 - acc: 0.9649"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model_gen, fe_type, k=5, params_dict=None):\n",
    "    histories = []\n",
    "    losses = []\n",
    "    accs = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        print(f'Training model {i+1}')\n",
    "        \n",
    "        set_seed()\n",
    "        \n",
    "        train_df, val_df, test_df = split_train_val_test(SEED+i)\n",
    "        train_df = balance_data_by_resample(train_df)\n",
    "        params_dict['steps'] = train_df.shape[0] // params_dict['batch_size']\n",
    "        params_dict['validation_steps'] = val_df.shape[0] // params_dict['batch_size']\n",
    "        curr_model, curr_history = train_model(model_gen, fe_type, train_df, val_df, params_dict=params_dict)\n",
    "        \n",
    "        \n",
    "        test_flow = datagen_flow(test_df, params_dict['batch_size'], fe_type, 'test')\n",
    "        curr_loss, curr_acc = curr_model.evaluate(test_flow, steps=params_dict['validation_steps'])\n",
    "        \n",
    "        histories.append(curr_history)\n",
    "        losses.append(curr_loss)\n",
    "        accs.append(curr_acc)\n",
    "        \n",
    "        del curr_model\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "    print(f'Test CrossEntropy: {np.mean(losses):.4f} +-{np.std(losses):.4f}')\n",
    "    print(f'Test Accuracy: {np.mean(accs)*100:.2f}% +-{np.std(accs)*100:.2f}%')\n",
    "    \n",
    "    return histories\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visualize_perf(history_per_fold):\n",
    "    k = len(history_per_fold)\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=k, figsize=(k*5,10))\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    fig.suptitle(f'Model performance over epochs')\n",
    "    \n",
    "    for fold_no, h in enumerate(history_per_fold):\n",
    "\n",
    "        for k in ['loss', 'val_loss']:\n",
    "            ax[0,fold_no].plot(h[k])\n",
    "            ax[0,fold_no].legend(['train_loss', 'val_loss'])\n",
    "            ax[0,fold_no].margins(0.01)\n",
    "            ax[0,fold_no].set_title(f'Fold #{fold_no+1} - loss')\n",
    "\n",
    "        for k in ['acc', 'val_acc']:\n",
    "            ax[1,fold_no].plot(h[k])\n",
    "            ax[1,fold_no].legend(['train_acc', 'val_acc'])\n",
    "            ax[1,fold_no].margins(0.01)\n",
    "            ax[1,fold_no].set_title(f'Fold #{fold_no+1} - accuracy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "params_dict = {\n",
    "    'epochs': 20,\n",
    "    'batch_size': 32,\n",
    "}\n",
    "\n",
    "image_shape = (224, 224, 3)\n",
    "resnet_histories = evaluate_model(init_model, 'resnet', k=2, params_dict=params_dict)\n",
    "visualize_perf(resnet_histories)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "liav_env",
   "language": "python",
   "name": "liav_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
