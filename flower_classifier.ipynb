{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Transfer Learning\n",
    " \n",
    "**Authors:**\n",
    "\n",
    "1.   Liav Bachar 205888472\n",
    "2.   Naor Kolet 205533060\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2z5iHmg0oke"
   },
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "pT--0V_r0Q4B"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.applications import VGG16, ResNet50V2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import log_loss\n",
    " \n",
    "# Plots\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Misc.\n",
    "from scipy.io import loadmat\n",
    "import os\n",
    "import random\n",
    "import joblib\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another dataset? https://www.tensorflow.org/tutorials/load_data/images\n",
    "if not os.path.exists(r'./datasets/'):\n",
    "    !mkdir ./datasets\n",
    "    !wget 'https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz' -P './datasets/'\n",
    "    !tar -xf ./datasets/102flowers.tgz -C ./datasets/\n",
    "    \n",
    "#     !wget 'https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102segmentations.tgz' -P './datasets/'\n",
    "    \n",
    "    !wget 'https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat' -P './datasets/'\n",
    "    \n",
    "labels = loadmat('./datasets/imagelabels.mat')['labels'].reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = glob('./datasets/jpg/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val_test(split_seed):\n",
    "    # train 50% validation 25% test 25%\n",
    "    train_paths, val_tst_paths, train_labels, val_tst_labels = train_test_split(images_path, labels, train_size=0.5, random_state=split_seed, shuffle=True, stratify=labels)\n",
    "    val_paths, tst_paths, val_labels, tst_labels = train_test_split(val_tst_paths, val_tst_labels, train_size=0.5, random_state=split_seed, shuffle=True, stratify=val_tst_labels)\n",
    "    \n",
    "    def convert2df(paths, labels): return pd.DataFrame({'filename': paths, 'class':labels}).astype(str)\n",
    "    train_df = convert2df(train_paths, train_labels)\n",
    "    val_df = convert2df(val_paths, val_labels)\n",
    "    test_df = convert2df(tst_paths, tst_labels)\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = split_train_val_test(split_seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagen_flow(df, batch_size, resize_shape, subset):\n",
    "    classes = list(np.unique(labels).astype(str))\n",
    "    \n",
    "    if subset == 'trainy':\n",
    "        data_gen = ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            rescale=1./255)\n",
    "    else:\n",
    "        data_gen = ImageDataGenerator(rescale=1./255)\n",
    "        \n",
    "    return data_gen.flow_from_dataframe(df, batch_size=batch_size, target_size=resize_shape, seed=SEED, validate_filenames=False, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4094 non-validated image filenames belonging to 102 classes.\n",
      "Found 2047 non-validated image filenames belonging to 102 classes.\n",
      "Found 2048 non-validated image filenames belonging to 102 classes.\n"
     ]
    }
   ],
   "source": [
    "classes = list(np.unique(labels).astype(str))\n",
    "data_loader_train = datagen_flow(train_df, 32, (224,224), 'train')\n",
    "data_loader_val = datagen_flow(val_df, 32, (224,224), 'val')\n",
    "data_loader_test = datagen_flow(test_df, 32, (224,224), 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 224, 224, 3), (32, 102))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = data_loader_train.next()\n",
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_extractor(base_model, width=224, height=224, channel=3):\n",
    "    model = base_model(input_shape=(width, height, channel), include_top=False)\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    return Model(model.input, model.output, name=base_model.__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_fe = get_feature_extractor(VGG16)\n",
    "resent_fe = get_feature_extractor(ResNet50V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Here we build our model architecture. It contains 4 main parts.\n",
    "\n",
    "1. **Embedding Layer** - Here the words of each song will be converted to their embeddings representations. We made this layer trainable, let the model learn the embeddings for the missing words.\n",
    "2. **Concatenate Layer** - Here we attach for each word entry the related melody record\n",
    "3. **LSTM Layers** - Here we created two layers of lstm, the first one learn features across the windows and outputs all the hidden states, the next one gets those hidden states and output a single feature that was learnt from them.\n",
    "4. **Softmax Layer** - Give each word in our dictionary the probabilty it will be the next word after the window.\n",
    "\n",
    "Note: We are using Dropout layers in order to reduce the overfitting -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(image_shape, output_shape, fe_type='vgg'):\n",
    "    inp = Input(shape=image_shape, name='image')\n",
    "    \n",
    "    if fe_type == 'vgg':\n",
    "        X = get_feature_extractor(VGG16)(inp)\n",
    "    elif fe_type == 'resnet':\n",
    "        X = get_feature_extractor(ResNet50V2)(inp)\n",
    "    else:\n",
    "        X = inp\n",
    "\n",
    "    X = Conv2D(256, 3, activation='relu', padding='same')(X)\n",
    "    X = Conv2D(128, 3, activation='relu', padding='same')(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(4096, activation=\"relu\")(X)\n",
    "    X = Dense(2048, activation=\"relu\")(X)\n",
    "    X = Dense(1024, activation=\"relu\")(X)\n",
    "    out = Dense(output_shape, activation=\"softmax\", name = 'out')(X)\n",
    "\n",
    "    model = Model(inp, out)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image (InputLayer)           [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "ResNet50V2 (Model)           (None, 7, 7, 2048)        23564800  \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 256)         4718848   \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 7, 7, 128)         295040    \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 4096)              25694208  \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 2048)              8390656   \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 102)               104550    \n",
      "=================================================================\n",
      "Total params: 64,866,278\n",
      "Trainable params: 41,301,478\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_shape = (224, 224, 3)\n",
    "output_shape = len(classes)\n",
    "model = init_model(image_shape, output_shape, fe_type='resnet')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(model_name):\n",
    "    acc = 'val_loss'\n",
    "    acc_mode = 'min'\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "                              fr'./models/{model_name}.h5', \n",
    "                              monitor=acc, \n",
    "#                               verbose=1, \n",
    "                              save_best_only=True, \n",
    "                              mode=acc_mode)\n",
    "    earlystop = EarlyStopping(monitor=acc, mode=acc_mode, verbose=1, patience=6)\n",
    "    reduceLR = ReduceLROnPlateau(monitor = 'val_loss', mode = 'min', patience = 5,\n",
    "                            factor = 0.5, min_lr = 1e-6, verbose = 1)\n",
    "\n",
    "    return [reduceLR, earlystop] #checkpoint, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_gen, fe_type, train_df, val_df, use_saved=False, params_dict=None):\n",
    "    os.makedirs('./models', exist_ok=True)\n",
    "    params = ''\n",
    "    if params_dict is not None:\n",
    "        params = '_'.join(f'{key}_{val}' for key,val in params_dict.items())\n",
    "    model_name = f'{fe_type}' + f'_{params}'\n",
    "        \n",
    "    if use_saved:\n",
    "        history = joblib.load(fr'./models/{model_name}_history.sav')\n",
    "    else:\n",
    "        callbacks = get_callbacks(model_name)\n",
    "        \n",
    "        train_flow = datagen_flow(train_df, params_dict['batch_size'], image_shape[:-1], 'train')\n",
    "        val_flow = datagen_flow(val_df, params_dict['batch_size'], image_shape[:-1], 'val')\n",
    "        \n",
    "        model = model_gen(image_shape, output_shape, fe_type)\n",
    "        history = model.fit(\n",
    "                            x=train_flow,\n",
    "                            y=None,\n",
    "                            batch_size=params_dict['batch_size'],\n",
    "                            epochs=params_dict['epochs'],\n",
    "                            validation_data=val_flow,\n",
    "                            callbacks=callbacks,\n",
    "                            steps_per_epoch = params_dict['steps'],\n",
    "                            validation_steps = params_dict['steps'],\n",
    "                            )\n",
    "        \n",
    "        history = history.history\n",
    "        joblib.dump(history, fr'./models/{model_name}_history.sav')\n",
    "    \n",
    "#     model = load_model(fr'./models/{model_name}.h5')\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_perf(history):\n",
    "    ncols = 1\n",
    "    fig, ax = plt.subplots(ncols=ncols, figsize=(5*ncols,5))\n",
    "    fig.suptitle(f'Model performance over epochs')\n",
    "    \n",
    "    for k in ['val_loss', 'loss']:\n",
    "        ax.plot(history[k])\n",
    "        \n",
    "    ax.legend(['val_loss', 'train_loss'])\n",
    "    ax.margins(0.01)\n",
    "    ax.set_title('binary crossentropy')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4094 non-validated image filenames belonging to 102 classes.\n",
      "Found 2047 non-validated image filenames belonging to 102 classes.\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "params_dict = {\n",
    "    'epochs': 20,\n",
    "    'batch_size': 32,\n",
    "    'steps': 256\n",
    "}\n",
    "\n",
    "vgg_model, vgg_history = train_model(init_model, 'resnet', train_df, val_df, params_dict=params_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-env)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
