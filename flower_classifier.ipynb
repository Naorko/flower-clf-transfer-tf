{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Transfer Learning\n",
    " \n",
    "**Authors:**\n",
    "\n",
    "1.   Liav Bachar 205888472\n",
    "2.   Naor Kolet 205533060\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2z5iHmg0oke"
   },
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pT--0V_r0Q4B"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from tensorflow.keras.applications import VGG16, ResNet50V2, EfficientNetB4\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import log_loss\n",
    " \n",
    "# Plots\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Misc.\n",
    "from scipy.io import loadmat\n",
    "import os\n",
    "import random\n",
    "import joblib\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another dataset? https://www.tensorflow.org/tutorials/load_data/images\n",
    "if not os.path.exists(r'./datasets/'):\n",
    "    !mkdir ./datasets\n",
    "    !wget 'https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz' -P './datasets/'\n",
    "    !tar -xf ./datasets/102flowers.tgz -C ./datasets/\n",
    "    \n",
    "#     !wget 'https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102segmentations.tgz' -P './datasets/'\n",
    "    \n",
    "    !wget 'https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat' -P './datasets/'\n",
    "    \n",
    "labels = loadmat('./datasets/imagelabels.mat')['labels'].reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = glob('./datasets/jpg/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val_test(split_seed):\n",
    "    # train 50% validation 25% test 25%\n",
    "    train_paths, val_tst_paths, train_labels, val_tst_labels = train_test_split(images_path, labels, train_size=0.5, random_state=split_seed, shuffle=True, stratify=labels)\n",
    "    val_paths, tst_paths, val_labels, tst_labels = train_test_split(val_tst_paths, val_tst_labels, train_size=0.5, random_state=split_seed, shuffle=True, stratify=val_tst_labels)\n",
    "    \n",
    "    def convert2df(paths, labels): return pd.DataFrame({'filename': paths, 'class':labels}).astype(str)\n",
    "    train_df = convert2df(train_paths, train_labels)\n",
    "    val_df = convert2df(val_paths, val_labels)\n",
    "    test_df = convert2df(tst_paths, tst_labels)\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = split_train_val_test(split_seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagen_flow(df, batch_size, resize_shape, subset):\n",
    "    classes = list(np.unique(labels).astype(str))\n",
    "    \n",
    "    if subset == 'train':\n",
    "        data_gen = ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "#              rescale=1./255\n",
    "            preprocessing_function=preprocess_input\n",
    "           )\n",
    "    else:\n",
    "        data_gen = ImageDataGenerator(rescale=1./255)\n",
    "        \n",
    "    return data_gen.flow_from_dataframe(df, batch_size=batch_size, target_size=resize_shape, seed=SEED, validate_filenames=False, classes=classes, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_extractor(base_model, width=224, height=224, channel=3):\n",
    "    model = base_model(input_shape=(width, height, channel), include_top=False)\n",
    "    \n",
    "    trainable = False\n",
    "    for layer in model.layers:\n",
    "        if base_model.__name__ == 'ResNet50V2' and 'conv5' in layer.name:\n",
    "            trainable = True\n",
    "#         if base_model.__name__ == 'EfficientNetB4' and 'conv5' in layer.name\n",
    "        \n",
    "        layer.trainable = trainable\n",
    "        \n",
    "            \n",
    "    return Model(model.input, model.output, name=base_model.__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vgg_fe = get_feature_extractor(VGG16)\n",
    "resent_fe = get_feature_extractor(ResNet50V2)\n",
    "efficient_net = get_feature_extractor(EfficientNetB4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resent_fe.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Here we build our model architecture. It contains 4 main parts.\n",
    "\n",
    "1. **Embedding Layer** - Here the words of each song will be converted to their embeddings representations. We made this layer trainable, let the model learn the embeddings for the missing words.\n",
    "2. **Concatenate Layer** - Here we attach for each word entry the related melody record\n",
    "3. **LSTM Layers** - Here we created two layers of lstm, the first one learn features across the windows and outputs all the hidden states, the next one gets those hidden states and output a single feature that was learnt from them.\n",
    "4. **Softmax Layer** - Give each word in our dictionary the probabilty it will be the next word after the window.\n",
    "\n",
    "Note: We are using Dropout layers in order to reduce the overfitting -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_model(image_shape, output_shape, fe_type='vgg'):\n",
    "    inp = Input(shape=image_shape, name='image')\n",
    "    \n",
    "    if fe_type == 'vgg':\n",
    "        X = get_feature_extractor(VGG16)(inp)\n",
    "    elif fe_type == 'resnet':\n",
    "        X = get_feature_extractor(ResNet50V2)(inp)\n",
    "    else:\n",
    "        X = inp\n",
    "    \n",
    "#     X = Conv2D(16, 5, activation='relu', padding='same')(X)\n",
    "    X = Flatten()(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(4096, activation=\"relu\")(X) #\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(2048, activation=\"relu\")(X) #, kernel_regularizer=tf.keras.regularizers.L2(0.01)\n",
    "    out = Dense(output_shape, activation=\"softmax\", name = 'out')(X)\n",
    "\n",
    "    model = Model(inp, out)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=1e-4), metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image (InputLayer)           [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "ResNet50V2 (Functional)      (None, 7, 7, 2048)        23564800  \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 16)          819216    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              3215360   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2048)              8390656   \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 102)               208998    \n",
      "=================================================================\n",
      "Total params: 36,202,166\n",
      "Trainable params: 27,606,678\n",
      "Non-trainable params: 8,595,488\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_shape = (224, 224, 3)\n",
    "output_shape = len(classes)\n",
    "model = init_model(image_shape, output_shape, fe_type='resnet')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(model_name):\n",
    "    acc = 'val_acc'\n",
    "    acc_mode = 'max'\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "                              fr'./models/{model_name}.h5', \n",
    "                              monitor=acc, \n",
    "#                               verbose=1, \n",
    "                              save_best_only=True, \n",
    "                              mode=acc_mode)\n",
    "    earlystop = EarlyStopping(monitor=acc, mode=acc_mode, verbose=1, patience=4)\n",
    "    reduceLR = ReduceLROnPlateau(monitor = 'val_loss', mode = 'min', patience = 3,\n",
    "                            factor = 0.5, min_lr = 1e-6, verbose = 1)\n",
    "\n",
    "    return [checkpoint] #reduceLR, earlystop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_gen, fe_type, train_df, val_df, use_saved=False, params_dict=None):\n",
    "    os.makedirs('./models', exist_ok=True)\n",
    "    params = ''\n",
    "    if params_dict is not None:\n",
    "        params = '_'.join(f'{key}_{val}' for key,val in params_dict.items())\n",
    "    model_name = f'{fe_type}' + f'_{params}'\n",
    "        \n",
    "    if use_saved:\n",
    "        history = joblib.load(fr'./models/{model_name}_history.sav')\n",
    "    else:\n",
    "        callbacks = get_callbacks(model_name)\n",
    "        \n",
    "        train_flow = datagen_flow(train_df, params_dict['batch_size'], image_shape[:-1], 'train')\n",
    "        val_flow = datagen_flow(val_df, params_dict['batch_size'], image_shape[:-1], 'val')\n",
    "        \n",
    "        model = model_gen(image_shape, output_shape, fe_type)\n",
    "        history = model.fit(\n",
    "                            x=train_flow,\n",
    "                            y=None,\n",
    "                            batch_size=None,\n",
    "                            epochs=params_dict['epochs'],\n",
    "                            validation_data=val_flow,\n",
    "                            callbacks=callbacks,\n",
    "                            steps_per_epoch = params_dict['steps'],\n",
    "                            validation_steps = params_dict['validation_steps'],\n",
    "                            workers=10\n",
    "                            )\n",
    "        \n",
    "        history = history.history\n",
    "        joblib.dump(history, fr'./models/{model_name}_history.sav')\n",
    "    \n",
    "#     model = load_model(fr'./models/{model_name}.h5')\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_perf(history):\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(5*2,5))\n",
    "    fig.suptitle(f'Model performance over epochs')\n",
    "    \n",
    "    for k in ['loss', 'val_loss']:\n",
    "        ax[0].plot(history[k])\n",
    "        \n",
    "    ax[0].legend(['train_loss', 'val_loss'])\n",
    "    ax[0].margins(0.01)\n",
    "    ax[0].set_title('Crossentropy')\n",
    "    \n",
    "    for k in ['acc', 'val_acc']:\n",
    "        ax[1].plot(history[k])\n",
    "        \n",
    "    ax[1].legend(['train_accuracy', 'val_accuracy'])\n",
    "    ax[1].margins(0.01)\n",
    "    ax[1].set_title('Accuracy')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4094 non-validated image filenames belonging to 102 classes.\n",
      "Found 2047 non-validated image filenames belonging to 102 classes.\n",
      "Epoch 1/20\n",
      "255/255 [==============================] - 53s 185ms/step - loss: 40.5701 - acc: 0.0189 - val_loss: 28.2742 - val_acc: 0.0276\n",
      "Epoch 2/20\n",
      "255/255 [==============================] - 29s 112ms/step - loss: 24.9962 - acc: 0.0304 - val_loss: 17.2151 - val_acc: 0.0276\n",
      "Epoch 3/20\n",
      "255/255 [==============================] - 43s 166ms/step - loss: 15.2216 - acc: 0.0434 - val_loss: 10.9416 - val_acc: 0.0207\n",
      "Epoch 4/20\n",
      "255/255 [==============================] - 28s 104ms/step - loss: 9.8182 - acc: 0.0403 - val_loss: 7.7508 - val_acc: 0.0207\n",
      "Epoch 5/20\n",
      "255/255 [==============================] - 37s 141ms/step - loss: 6.9902 - acc: 0.0528 - val_loss: 6.1538 - val_acc: 0.0212\n",
      "Epoch 6/20\n",
      "255/255 [==============================] - 49s 187ms/step - loss: 5.5790 - acc: 0.0545 - val_loss: 5.4200 - val_acc: 0.0236\n",
      "Epoch 7/20\n",
      "255/255 [==============================] - 26s 100ms/step - loss: 4.8863 - acc: 0.0741 - val_loss: 5.1958 - val_acc: 0.0202\n",
      "Epoch 8/20\n",
      "255/255 [==============================] - 27s 103ms/step - loss: 4.5505 - acc: 0.0789 - val_loss: 5.1033 - val_acc: 0.0197\n",
      "Epoch 9/20\n",
      "255/255 [==============================] - 27s 101ms/step - loss: 4.3471 - acc: 0.1016 - val_loss: 5.2318 - val_acc: 0.0207\n",
      "Epoch 10/20\n",
      "255/255 [==============================] - 28s 107ms/step - loss: 4.2042 - acc: 0.1294 - val_loss: 5.4066 - val_acc: 0.0276\n",
      "Epoch 11/20\n",
      "255/255 [==============================] - 27s 102ms/step - loss: 4.0794 - acc: 0.1413 - val_loss: 5.5101 - val_acc: 0.0177\n",
      "Epoch 12/20\n",
      "255/255 [==============================] - 27s 103ms/step - loss: 3.8754 - acc: 0.1762 - val_loss: 5.7946 - val_acc: 0.0177\n",
      "Epoch 13/20\n",
      "255/255 [==============================] - 28s 105ms/step - loss: 3.7974 - acc: 0.1844 - val_loss: 6.2739 - val_acc: 0.0167\n",
      "Epoch 14/20\n",
      "255/255 [==============================] - 27s 104ms/step - loss: 3.6402 - acc: 0.2170 - val_loss: 6.5551 - val_acc: 0.0128\n",
      "Epoch 15/20\n",
      "167/255 [==================>...........] - ETA: 7s - loss: 3.3561 - acc: 0.2675"
     ]
    }
   ],
   "source": [
    "params_dict = {\n",
    "    'epochs': 20,\n",
    "    'batch_size': 16,\n",
    "    'steps': 255,\n",
    "    'validation_steps':127\n",
    "}\n",
    "\n",
    "vgg_model, vgg_history = train_model(init_model, 'resnet', train_df, val_df, params_dict=params_dict)\n",
    "\n",
    "# del vgg_model\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_perf(vgg_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "liav_env",
   "language": "python",
   "name": "liav_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
